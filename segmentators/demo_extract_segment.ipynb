{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac1df5d",
   "metadata": {},
   "source": [
    "# Demonstration of `extract_protected_spans` and `segment_tokens`\n",
    "This notebook demonstrates how to use the `extract_protected_spans` and `segment_tokens` functions from the `code_segmentation.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b72163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path().resolve().parent\n",
    "sys.path.insert(0, str(root))\n",
    "\n",
    "from pretokenizers.firstpretokenizer import FirstPretokenizer \n",
    "from segmentators.ultimatesegmentator import UltimateSegmentator\n",
    "from utils.pretty_printer import pretty_print_tokens, pretty_print_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a48fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenizer = FirstPretokenizer(_use_dedent=True, _use_semantics=True)\n",
    "segmentator = UltimateSegmentator(pretokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f551acd",
   "metadata": {},
   "source": [
    "## Step 1: Define and parse a code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5f0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEF] add [DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "[CLASS] Calculator [BLOCK]\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n"
     ]
    }
   ],
   "source": [
    "code_example = '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class Calculator:\n",
    "    def subtract(self, x, y):\n",
    "        return x - y\n",
    "'''\n",
    "\n",
    "parsed = pretokenizer.pretokenize(ast.parse(code_example))\n",
    "tokens = segmentator.tokenize_pretokenized_string(parsed)\n",
    "pretty_print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b1633",
   "metadata": {},
   "source": [
    "## Step 2: Extract protected spans using `extract_protected_spans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b037be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "\n",
      "=== Span (0, 21) ===\n",
      "[DEF] add [DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (2, 10) ===\n",
      "[DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R]\n",
      "\n",
      "=== Span (12, 20) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "\n",
      "=== Span (21, 24) ===\n",
      "[DEDENT]\n",
      "[CLASS] Calculator [BLOCK]\n",
      "\n",
      "=== Span (22, 52) ===\n",
      "[CLASS] Calculator [BLOCK]\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (25, 41) ===\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "\n",
      "=== Span (25, 52) ===\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (28, 40) ===\n",
      "[DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R]\n",
      "\n",
      "=== Span (42, 50) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "\n",
      "=== Span (51, 52) ===\n",
      "[DEDENT]\n",
      "[DEDENT]\n"
     ]
    }
   ],
   "source": [
    "spans = segmentator.extract_protected_spans(tokens, all_options=True)\n",
    "pretty_print_spans(tokens, spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d885",
   "metadata": {},
   "source": [
    "## Step 3: Segment tokens using `segment_tokens`\n",
    "We split the token sequence into segments of at most 10 tokens, avoiding cuts within protected spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fda08e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment [0:21]: ['[DEF]', 'add', '[DELIMIT_1_L]', '[SEMANTIC_START]', 'a', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'b', '[SEMANTIC_END]', '[DELIMIT_1_R]', '[BLOCK]', '[INDENT]', '[RETURN]', '[SEMANTIC_START]', 'a', '[SEMANTIC_END]', '[ADD]', '[SEMANTIC_START]', 'b', '[SEMANTIC_END]']\n",
      "Segment [21:53]: ['[DEDENT]', '[CLASS]', 'Calculator', '[BLOCK]', '[INDENT]', '[DEF]', 'subtract', '[DELIMIT_1_L]', '[SEMANTIC_START]', 'self', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'x', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'y', '[SEMANTIC_END]', '[DELIMIT_1_R]', '[BLOCK]', '[INDENT]', '[RETURN]', '[SEMANTIC_START]', 'x', '[SEMANTIC_END]', '[SUB]', '[SEMANTIC_START]', 'y', '[SEMANTIC_END]', '[DEDENT]', '[DEDENT]']\n"
     ]
    }
   ],
   "source": [
    "segments = segmentator.segment_tokens(tokens, max_len=40, protected_spans=spans)\n",
    "for start, end in segments: \n",
    "    print(f\"Segment [{start}:{end}]:\", tokens[start:end])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
