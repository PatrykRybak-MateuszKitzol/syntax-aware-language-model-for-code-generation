{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from data_processing.preprocessors.preprocessor import PreprocessingPipeline\n",
    "from data_processing.preprocessors.preprocessor import RemoveComments\n",
    "from data_processing.preprocessors.preprocessor import SyntaxValidator\n",
    "from data_processing.preprocessors.preprocessor import DuplicateFilter\n",
    "from data_processing.preprocessors.preprocessor import Pep8Formatter\n",
    "\n",
    "from data_processing.pretokenizers.firstpretokenizer import FirstPretokenizer\n",
    "\n",
    "from data_processing.utils.pretokenize_all import pretokenize_all\n",
    "from data_processing.utils.docstring_and_code_filtering import doctring_and_code_filtering\n",
    "from data_processing.utils.data_loader import load_and_split_dataset\n",
    "from data_processing.utils.data_preparation import preprocess\n",
    "\n",
    "from training.utils.model_utils import load_tokenizer\n",
    "\n",
    "from config import (\n",
    "    RUN_SEGEMENTATOR,\n",
    "    MODEL_NAME,\n",
    "    MAX_INPUT_LENGTH,\n",
    "    MAX_OUTPUT_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple yet powerfull step by step guideline for reproducing our results. The repository contains code for experimenting with a custom training method for the T5 language model, aimed at improving its performance in code generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and truncate the raw dataset to the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before cleaning: 10\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Nan-Do/code-search-net-python\", token=access_token)[\"train\"]\n",
    "dataset = dataset.select(range(10))\n",
    "print(\"Number of samples before cleaning:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the samples by removing comments, formatting them to the PEP8 standard, and getting rid of syntactically incorrect samples and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after cleaning: 10\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline([\n",
    "    RemoveComments(),\n",
    "    DuplicateFilter(),\n",
    "    Pep8Formatter(),\n",
    "    SyntaxValidator()\n",
    "])\n",
    "\n",
    "data = pipeline.apply(dataset)\n",
    "\n",
    "print(\"Number of samples after cleaning:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and apply the pretokenizer.\n",
    "\n",
    "The pretokenizer is simply an AST visitor that walks through the code structure and changes its string representation to match special tokens in the tokenizer, so they can be detected and translated into the proper token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenizer = FirstPretokenizer(_use_dedent=True, _use_semantics=True)\n",
    "data = pretokenize_all(data, pretokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and apply the semgmentator.\n",
    "\n",
    "It allows for \"masking\" and helps the model capture local dependencies. It replaces each sample with a few new ones that have consistent code fragments masked out, and the labels are adjusted accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SEGEMENTATOR:\n",
    "    data = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with â‰¤512 tokens: 10 / 10 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "data = doctring_and_code_filtering(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 8\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 1\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 1\n",
       " })}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = load_and_split_dataset(data)\n",
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step to complete data preparation is to adjust the sample length to the model's context window. We need to **load a tokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, specifics = load_tokenizer(MODEL_NAME, pretokenizer)\n",
    "semantic_start_id, semantic_end_id, code_token_ids, semantic_token_ids = specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization and length adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = {\n",
    "    split: dataset.map(\n",
    "        lambda batch: preprocess(batch, tokenizer, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    for split, dataset in dataset_dict.items()\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
