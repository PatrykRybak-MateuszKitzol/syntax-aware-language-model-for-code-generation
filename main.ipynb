{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patryk/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/patryk/Documents/syntax-aware-language-model-for-code-generation/model_operations/training/models/t5-base-split20-epochs2-lossTrue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-20 00:56:02.591318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747695362.611654   63662 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747695362.617671   63662 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747695362.633374   63662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747695362.633395   63662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747695362.633397   63662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747695362.633399   63662 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "from data_processing.preprocessors.preprocessor import PreprocessingPipeline\n",
    "from data_processing.preprocessors.preprocessor import RemoveComments\n",
    "from data_processing.preprocessors.preprocessor import SyntaxValidator\n",
    "from data_processing.preprocessors.preprocessor import DuplicateFilter\n",
    "from data_processing.preprocessors.preprocessor import Pep8Formatter\n",
    "\n",
    "from data_processing.pretokenizers.firstpretokenizer import FirstPretokenizer\n",
    "\n",
    "from data_processing.segmentators.ultimatesegmentator import UltimateSegmentator\n",
    "\n",
    "from data_processing.utils.pretokenize_all import pretokenize_all\n",
    "from data_processing.utils.docstring_and_code_filtering import doctring_and_code_filtering\n",
    "from data_processing.utils.data_loader import load_and_split_dataset\n",
    "from data_processing.utils.data_preparation import preprocess\n",
    "\n",
    "from model_operations.utils.model_utils import load_tokenizer\n",
    "\n",
    "from config import (\n",
    "    RUN_SEGEMENTATOR,\n",
    "    MODEL_NAME,\n",
    "    MAX_INPUT_LENGTH,\n",
    "    MAX_OUTPUT_LENGTH,\n",
    "    USE_CUSTOM_EOS,\n",
    "    EOS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = os.getenv(\"HUGGINGFACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple yet powerfull step by step guideline for reproducing our results. The repository contains code for experimenting with a custom training method for the T5 language model, aimed at improving its performance in code generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and truncate the raw dataset to the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Nan-Do/code-search-net-python\", token=access_token)[\"train\"]\n",
    "dataset = dataset.map(lambda x: {**x, \"code_length\": len(x[\"code\"])})\n",
    "dataset = dataset.sort(\"code_length\", reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of 'code_length' column:\n",
      "                   0\n",
      "count  455243.000000\n",
      "mean     1058.562045\n",
      "std      1412.836589\n",
      "min        75.000000\n",
      "25%       397.000000\n",
      "50%       666.000000\n",
      "75%      1208.000000\n",
      "max    103665.000000\n"
     ]
    }
   ],
   "source": [
    "code_lengths = dataset[\"code_length\"]\n",
    "\n",
    "print(\"Statistics of 'code_length' column:\")\n",
    "df = pd.DataFrame(code_lengths)\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples before cleaning: 30349\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.select(range(len(dataset) // 15))\n",
    "print(\"Number of samples before cleaning:\", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean the samples by removing comments, formatting them to the PEP8 standard, and getting rid of syntactically incorrect samples and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples after cleaning: 30074\n"
     ]
    }
   ],
   "source": [
    "pipeline = PreprocessingPipeline([\n",
    "    RemoveComments(),\n",
    "    DuplicateFilter(),\n",
    "    Pep8Formatter(),\n",
    "    SyntaxValidator()\n",
    "])\n",
    "\n",
    "data = pipeline.apply(dataset)\n",
    "\n",
    "print(\"Number of samples after cleaning:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing for another, high quality dataset adn then concatinate them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and apply the pretokenizer.\n",
    "\n",
    "The pretokenizer is simply an AST visitor that walks through the code structure and changes its string representation to match special tokens in the tokenizer, so they can be detected and translated into the proper token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenizer = FirstPretokenizer(_use_dedent=True, _use_semantics=True)\n",
    "data = pretokenize_all(data, pretokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need tokenizer for segmentation process and for a later use. We need to **load a tokenizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, specifics = load_tokenizer(MODEL_NAME, USE_CUSTOM_EOS, pretokenizer)\n",
    "if specifics:\n",
    "    semantic_start_id, semantic_end_id, code_token_ids, semantic_token_ids = specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and apply the semgmentator if specified.\n",
    "\n",
    "It allows for \"masking\" and helps the model capture local dependencies. It replaces each sample with a few new ones that have consistent code fragments masked out, and the labels are adjusted accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SEGEMENTATOR:\n",
    "    segmentator = UltimateSegmentator(pretokenizer)\n",
    "    data = segmentator.apply(data, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add custom eos token if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUSTOM_EOS:\n",
    "    for sample in data:\n",
    "        sample['parsed'] += EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples with â‰¤512 tokens: 224133 / 224133 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "data = doctring_and_code_filtering(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the results to use them in later external experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"50_smallest_docstring_and_code.jsonl\", 'w') as f:\n",
    "    for sample in data:\n",
    "        f.write(json.dumps(sample) + '\\n') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data into subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 748\n",
       " }),\n",
       " 'validation': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 94\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['docstring', 'parsed'],\n",
       "     num_rows: 94\n",
       " })}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = load_and_split_dataset(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step to complete data preparation is to adjust the sample length to the model's context window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform tokenization and length adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cde47394d64675b2ecbb3ec75b115b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db7534e082ee4feb8b951c676fe21b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1cf253146dd4131b7929d16fcc77520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/94 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = {\n",
    "    split: dataset.map(\n",
    "        lambda batch: preprocess(batch, tokenizer, USE_CUSTOM_EOS, MAX_INPUT_LENGTH, MAX_OUTPUT_LENGTH),\n",
    "        batched=True,\n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "    for split, dataset in dataset_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
