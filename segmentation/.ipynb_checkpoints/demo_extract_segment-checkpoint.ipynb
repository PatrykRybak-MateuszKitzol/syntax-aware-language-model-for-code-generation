{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ac1df5d",
   "metadata": {},
   "source": [
    "# Demonstration of `extract_protected_spans` and `segment_tokens`\n",
    "This notebook demonstrates how to use the `extract_protected_spans` and `segment_tokens` functions from the `code_segmentation.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b72163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "from code_segmentation import pretokenize, tokenize_pretokenized_string, extract_protected_spans, segment_tokens, pretty_print_tokens, pretty_print_spans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f551acd",
   "metadata": {},
   "source": [
    "## Step 1: Define and parse a code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a5f0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEF] add [DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "[CLASS] Calculator [BLOCK]\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n"
     ]
    }
   ],
   "source": [
    "code_example = '''\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "class Calculator:\n",
    "    def subtract(self, x, y):\n",
    "        return x - y\n",
    "'''\n",
    "\n",
    "parsed = pretokenize(ast.parse(code_example), _use_dedent=True, _use_semantics=True)\n",
    "tokens = tokenize_pretokenized_string(parsed)\n",
    "pretty_print_tokens(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b1633",
   "metadata": {},
   "source": [
    "## Step 2: Extract protected spans using `extract_protected_spans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b037be14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Span (0, 21) ===\n",
      "[DEF] add [DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (2, 10) ===\n",
      "[DELIMIT_1_L] [SEMANTIC_START] a [SEMANTIC_END] [COMMA] [SEMANTIC_START] b [SEMANTIC_END] [DELIMIT_1_R]\n",
      "\n",
      "=== Span (12, 20) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "\n",
      "=== Span (12, 21) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (13, 20) ===\n",
      "[RETURN]\n",
      "[SEMANTIC_START] a [SEMANTIC_END] [ADD] [SEMANTIC_START] b [SEMANTIC_END]\n",
      "\n",
      "=== Span (21, 24) ===\n",
      "[DEDENT]\n",
      "[CLASS] Calculator [BLOCK]\n",
      "\n",
      "=== Span (22, 52) ===\n",
      "[CLASS] Calculator [BLOCK]\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (25, 41) ===\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "\n",
      "=== Span (25, 52) ===\n",
      "[INDENT]\n",
      "    [DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "    [INDENT]\n",
      "        [RETURN]\n",
      "        [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "    [DEDENT]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (26, 51) ===\n",
      "[DEF] subtract [DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R] [BLOCK]\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (28, 40) ===\n",
      "[DELIMIT_1_L] [SEMANTIC_START] self [SEMANTIC_END] [COMMA] [SEMANTIC_START] x [SEMANTIC_END] [COMMA] [SEMANTIC_START] y [SEMANTIC_END] [DELIMIT_1_R]\n",
      "\n",
      "=== Span (42, 50) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "\n",
      "=== Span (42, 51) ===\n",
      "[INDENT]\n",
      "    [RETURN]\n",
      "    [SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "[DEDENT]\n",
      "\n",
      "=== Span (43, 50) ===\n",
      "[RETURN]\n",
      "[SEMANTIC_START] x [SEMANTIC_END] [SUB] [SEMANTIC_START] y [SEMANTIC_END]\n",
      "\n",
      "=== Span (51, 52) ===\n",
      "[DEDENT]\n",
      "[DEDENT]\n"
     ]
    }
   ],
   "source": [
    "spans = extract_protected_spans(tokens, all_options=True)\n",
    "pretty_print_spans(tokens, spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce9d885",
   "metadata": {},
   "source": [
    "## Step 3: Segment tokens using `segment_tokens`\n",
    "We split the token sequence into segments of at most 10 tokens, avoiding cuts within protected spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda08e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment [0:22]: ['[DEF]', 'add', '[DELIMIT_1_L]', '[SEMANTIC_START]', 'a', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'b', '[SEMANTIC_END]', '[DELIMIT_1_R]', '[BLOCK]', '[INDENT]', '[RETURN]', '[SEMANTIC_START]', 'a', '[SEMANTIC_END]', '[ADD]', '[SEMANTIC_START]', 'b', '[SEMANTIC_END]', '[DEDENT]']\n",
      "Segment [22:53]: ['[CLASS]', 'Calculator', '[BLOCK]', '[INDENT]', '[DEF]', 'subtract', '[DELIMIT_1_L]', '[SEMANTIC_START]', 'self', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'x', '[SEMANTIC_END]', '[COMMA]', '[SEMANTIC_START]', 'y', '[SEMANTIC_END]', '[DELIMIT_1_R]', '[BLOCK]', '[INDENT]', '[RETURN]', '[SEMANTIC_START]', 'x', '[SEMANTIC_END]', '[SUB]', '[SEMANTIC_START]', 'y', '[SEMANTIC_END]', '[DEDENT]', '[DEDENT]']\n"
     ]
    }
   ],
   "source": [
    "segments = segment_tokens(tokens, max_len=40, protected_spans=spans)\n",
    "for start, end in segments: \n",
    "    print(f\"Segment [{start}:{end}]:\", tokens[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33d82cb-f535-422b-98c0-bc507df513f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
